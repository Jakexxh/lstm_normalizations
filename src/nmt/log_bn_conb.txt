# Job id 0
# Loading hparams from /home/xxh/log/nmt_noattention_model_bn_conb/bn_sep_5.0_1.0/hparams
  saving hparams to /home/xxh/log/nmt_noattention_model_bn_conb/bn_sep_5.0_1.0/hparams
  saving hparams to /home/xxh/log/nmt_noattention_model_bn_conb/bn_sep_5.0_1.0/best_bleu/hparams
  attention=scaled_luong
  attention_architecture=standard
  batch_size=128
  beam_width=0
  best_bleu=0
  best_bleu_dir=/home/xxh/log/nmt_noattention_model_bn_conb/bn_sep_5.0_1.0/best_bleu
  bpe_delimiter=None
  colocate_gradients_with_ops=True
  decay_factor=0.5
  decay_steps=1000
  dev_prefix=../../data/nmt_data/tst2012
  dropout=0.0
  encoder_type=bi
  eos=</s>
  epoch_step=0
  forget_bias=0.0
  grain=5.0
  infer_batch_size=32
  init_op=uniform
  init_weight=0.1
  learning_rate=1.0
  length_penalty_weight=0.0
  log_device_placement=False
  max_gradient_norm=5.0
  max_train=0
  metrics=[u'bleu']
  num_buckets=5
  num_embeddings_partitions=0
  num_gpus=1
  num_layers=2
  num_residual_layers=0
  num_train_steps=12000
  num_units=512
  optimizer=sgd
  out_dir=/home/xxh/log/nmt_noattention_model_bn_conb/bn_sep_5.0_1.0
  pass_hidden_state=True
  random_seed=None
  residual=False
  share_vocab=False
  sos=<s>
  source_reverse=False
  src=vi
  src_max_len=50
  src_max_len_infer=None
  src_vocab_file=../../data/nmt_data/vocab.vi
  src_vocab_size=7709
  start_decay_step=8000
  steps_per_external_eval=None
  steps_per_stats=100
  test_prefix=../../data/nmt_data/tst2013
  tgt=en
  tgt_max_len=50
  tgt_max_len_infer=None
  tgt_vocab_file=../../data/nmt_data/vocab.en
  tgt_vocab_size=17191
  time_major=True
  train_prefix=../../data/nmt_data/train
  unit_type=bn_sep
  vocab_prefix=../../data/nmt_data/vocab
# creating train graph ...
  num_bi_layers = 1, num_bi_residual_layers=0
  cell 0  bn_sep LSTM, forget_bias=0  DeviceWrapper, device=/gpu:0
  cell 0  bn_sep LSTM, forget_bias=0  DeviceWrapper, device=/gpu:0
